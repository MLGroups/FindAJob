## 一：数据结构
### 1.链表

*比较顺序表和链表的优缺点，说说它们分别在什么场景下使用？*

    链表：
     {
       缺点：需要多余的空间存储指针。
             存取某个元素速度慢，需要遍历。
       优点：插入元素和删除元素速度快。
      没有空间限制,存储元素的个数无上限,基本只与内存空间大小有关
     }

    顺序表：
    {
       优点：空间利用率高。
      存取某个元素速度快（直接靠下标即可）
       缺点：插入元素和删除元素存在元素移动,速度慢,耗时。
      有空间限制,当需要存取的元素个数可能多于顺序表的元素个数时,会出现"溢出"问题.当元素个数远少于预先分配的空间时,空间浪费巨大.
    }
    总结：频繁的插入、删除数据时应用链表。

题目：
1. 求单链表中结点的个数
2. 将单链表反转
3. 查找单链表中的倒数第K个结点（k > 0）
4. 查找单链表的中间结点
5. 从尾到头打印单链表
6. 已知两个单链表pHead1 和pHead2 各自有序，把它们合并成一个链表依然有序
7. 判断一个单链表中是否有环
8. 判断两个单链表是否相交
9. 求两个单链表相交的第一个节点
10. 已知一个单链表中存在环，求进入环中的第一个节点
11. 给出一单链表头指针pHead和一节点指针pToBeDeleted，O(1)时间复杂度删除节点pToBeDeleted

### 2.二叉树
已知两种遍历序列，求另一种遍历序列。

二叉树各度节点数关系：

    n:节点总数，n0～n2：度为0～2的节点数

    n = n0 + n1 + n2
    n = n1 + 2 * n2 + 1
    n0 = n2 + 1

### 3.栈和队列

### 4.内存中的堆栈

### 5.图



## 二：计算机网络
### 1.三次握手，四次挥手

### 2.TCP、UDP区别，优缺点




## 三：操作系统
### 1.进程和线程的区别、关系


## 四：数据库
1.索引的结构

2.范式

## 五：机器学习、深度学习
### 1.SVM
#### SVM的原理是什么？
SVM是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。（间隔最大是它有别于感知机）

（1）当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；

（2）当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；

（3）当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。


#### SVM为什么采用间隔最大化？
当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。

感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。

线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。

然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—>求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。

#### 为什么要将求解SVM的原始问题转换为其对偶问题？
一、是对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）

二、自然引入核函数，进而推广到非线性分类问题。

#### 为什么SVM要引入核函数？
当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

在学习预测中，只定义核函数K(x,y)，而不是显式的定义映射函数ϕ。因为特征空间维数可能很高，甚至可能是无穷维，因此直接计算ϕ(x)·ϕ(y)是比较困难的。相反，直接计算K(x,y)比较容易（即直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果）。

核函数的定义：K(x,y)=<ϕ(x),ϕ(y)>，即在特征空间的内积等于它们在原始样本空间中通过核函数K计算的结果。

除了 SVM 之外，任何将计算表示为数据点的内积的方法，都可以使用核方法进行非线性扩展。


#### 为什么SVM对缺失数据敏感？
这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略（决策树有）。而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。

#### SVM如何处理多分类问题？
一般有两种做法：一种是直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。

另外一种做法是间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。

一对多，就是对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。

svm一对一法（one-vs-one），针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。

### 2.梯度下降

### 3.防止过拟合

正则化
dropout
early-stopping
数据增强

### 4.偏差、方差

    低偏差低方差时，是我们所追求的效果，此时预测值正中靶心(最接近真实值)，且比较集中(方差小)。
    低偏差高方差时，预测值基本落在真实值周围，但很分散，此时方差较大，说明模型的稳定性不够好。
    高偏差低方差时，预测值与真实值有较大距离，但此时值很集中，方差小；模型的稳定性较好，但预测准确率不高，处于“一如既往地预测不准”的状态。
    高偏差高方差时，是我们最不想看到的结果，此时模型不仅预测不准确，而且还不稳定，每次预测的值都差别比较大。


### 5.梯度消失、爆炸

### 6.加速训练、最小化损失

Momentum、RMSprop、Adam

学习率衰减

Batch Normalization

### 7.GRU、LSTM

### 8.决策树：ID3、C4.5、CART
ID3:信息增益。多值偏向。

C4.5:信息增益率。

CART:基尼指数。

ID3算法缺点：

    ID3算法不能处理具有连续值的属性
    ID3算法不能处理属性具有缺失值的样本
    算法会生成很深的树，容易产生过拟合现象
    算法一般会优先选择有较多属性值的特征，因为属性值多的特征会有相对较大的信息增益

C4.5弥补了ID3中不能处理特征属性值连续的问题。
但是，对连续属性值需要扫描排序，会使C4.5性能下降。

### 9.Ensemble Learning：Bagging、Boosting、Statcking

AdaBoost、GDBT、Xtgboost、lightGBM

### 10.贝叶斯、朴素贝叶斯

朴素贝叶斯：各特征独立

### 11.聚类
K-MEANS、DBSCAN、Birch、MEAN-SHIFT

K-MEANS影响结果的因素：k值、中心初始值。

K-MEANS优点：简单。
K-MEANS缺点：对异常值敏感;需要提前确定k值;不一定是全局最优，只能保证局部最优;
结果不稳定。


### 12.PCA、LDA

### 13.核函数
线性核、多项式核、径向基核、sigmoid核、高斯核

### 14.判别式、生成式
对于输入x，类别标签y：
产生式模型估计它们的联合概率分布P(x,y)，
判别式模型估计条件概率分布P(y|x)。

产生式模型可以根据贝叶斯公式得到判别式模型，但反过来不行。

判别式模型常见的主要有：

    Logistic Regression
    SVM
    Traditional Neural Networks
    Nearest Neighbor
    CRF
    Linear Discriminant Analysis
    Boosting
    Linear Regression

产生式模型常见的主要有：

    Gaussians
    Naive Bayes
    Mixtures of Multinomials
    Mixtures of Gaussians
    Mixtures of Experts
    HMMs
    Sigmoidal Belief Networks, Bayesian Networks
    Markov Random Fields
    Latent Dirichlet Allocation



## 六：数学
### 1.先验概率、后验概率

### 2.分布

### 3.检验

### 4.傅里叶变换

### 5.泰勒展开

### 6.对偶问题

### 7.正定矩阵
（1）广义定义：设M是n阶方阵，如果对任何非零向量z，都有zTMz> 0，其中zT 表示z的转置，就称M正定矩阵。

例如：B为n阶矩阵，E为单位矩阵，a为正实数。aE+B在a充分大时，aE+B为正定矩阵。（B必须为对称阵）

（2）狭义定义：一个n阶的实对称矩阵M是正定的的条件是当且仅当对于所有的非零实系数向量z，都有zTMz> 0。其中zT表示z的转置。




## 七：大数据
hadoop：
https://blog.csdn.net/qq_15103197/article/details/78404541?fps=1&locationNum=7

https://www.cnblogs.com/juncaoit/p/6421806.html