### 1.自我介绍

### 2.项目介绍

## 3.XGBoost
XGBoost实现的是一种通用的Tree Boosting算法，此算法的一个代表为梯度提升决策树（Gradient Boosting Decision Tree, GBDT），又名MART(Multiple Additive Regression Tree)。

GBDT的原理是，首先使用训练集和样本真值（即标准答案）训练一棵树，然后使用这棵树预测训练集，得到每个样本的预测值，由于预测值与真值存在偏差，所以二者相减可以得到“残差”。接下来训练第二棵树，此时不再使用真值，而是使用残差作为标准答案。两棵树训练完成后，可以再次得到每个样本的残差，然后进一步训练第三棵树，以此类推。树的总棵数可以人为指定，也可以监控某些指标（例如验证集上的误差）来停止训练。

在预测新样本时，每棵树都会有一个输出值，将这些输出值相加，即得到样本最终的预测值。

#### 损失函数从平方损失推广到二阶可导的损失：

GBDT的核心在于后面的树拟合的是前面预测值的残差，这样可以一步步逼近真值。然而，之所以拟合残差可以逼近到真值，是因为使用了平方损失作为损失函数。

如果换成是其他损失函数，使用残差将不再能够保证逼近真值。XGBoost的方法是，将损失函数做泰勒展开到第二阶，使用前两阶作为改进的残差。可以证明，传统GBDT使用的残差是泰勒展开到一阶的结果，因此，GBDT是XGBoost的一个特例。

#### 加入了正则化项：
正则化方法是数学中用来解决不适定问题的一种方法，后来被引入机器学习领域。通俗的讲，正则化是为了限制模型的复杂度的。模型越复杂，就越有可能“记住”训练数据，导致训练误差达到很低，而测试误差却很高，也就是发生了“过拟合”。在机器学习领域，正则化项大多以惩罚函数的形式存在于目标函数中，也就是在训练时，不仅只顾最小化误差，同时模型复杂度也不能太高。

在决策树中，模型复杂度体现在树的深度上。XGBoost使用了一种替代指标，即叶子节点的个数。此外，与许多其他机器学习模型一样，XGBoost也加入了L2正则项，来平滑各叶子节点的预测值。

#### 支持列抽样
列抽样是指，训练每棵树时，不是使用所有特征，而是从中抽取一部分来训练这棵树。这种方法原本是用在随机森林中的，经过试验，使用在GBDT中同样有助于效果的提升。


## 4.模型保存
**xgboost**
model.save_model(),model.load_model()

**tensorflow**

  1.TensorFlow通过tf.train.Saver类实现神经网络模型的保存和提取。tf.train.Saver对象saver的save方法将TensorFlow模型保存到指定路径中，saver.save(sess,"Model/model.ckpt")。

checkpoint文件保存了一个录下多有的模型文件列表，model.ckpt.meta保存了TensorFlow计算图的结构信息，model.ckpt保存每个变量的取值，此处文件名的写入方式会因不同参数的设置而不同，但加载restore时的文件路径名是以checkpoint文件中的“model_checkpoint_path”值决定的。

  2.加载这个已保存的TensorFlow模型的方法是saver.restore(sess,"./Model/model.ckpt")，加载模型的代码中也要定义TensorFlow计算图上的所有运算并声明一个tf.train.Saver类，不同的是加载模型时不需要进行变量的初始化，而是将变量的取值通过保存的模型加载进来，注意加载路径的写法。若不希望重复定义计算图上的运算，可直接加载已经持久化的图，saver =tf.train.import_meta_graph("Model/model.ckpt.meta")。

### 5.RNN、LSTM介绍（基本结构，应用场景）

## 6.LSTM如何解决梯度消失、爆炸（从数学方面解释）
乘法--->加法。

### 7.什么时候可以开始实习、实习多久、一周几天



##############################################

答得太jb烂了。。。